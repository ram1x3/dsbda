Mapper---------------------------------- 
 
package maximum_log; 
 
import org.apache.hadoop.mapreduce.Mapper; 
 
import org.apache.hadoop.io.Text; 
 
import org.apache.hadoop.io.IntWritable; 
 
import java.util.StringTokenizer; 
 
import java.io.IOException; 
 
import java.lang.InterruptedException; 
 
public class mapper extends Mapper<Object,Text,Text,IntWritable>{ 
 
public void map(Object offset,Text key,Context con) throws 
IOException,InterruptedException 
 
{ 
 
StringTokenizer token = new StringTokenizer(key.toString()," - - "); 
 
//while(token.hasMoreElements()) 
 
//{ 
 
con.write(new Text(token.nextToken()),new IntWritable(1)); 
 
//} 
 
} 
 
} 
 
 
reducer---------------------------------- 
 
package maximum_log; 
 
import org.apache.hadoop.mapreduce.Reducer; 
 
import org.apache.hadoop.io.Text; 
 
import org.apache.hadoop.io.IntWritable; 
 
import java.io.IOException; 
 
import java.lang.InterruptedException; 
 
public class reducer extends Reducer<Text,IntWritable,Text,IntWritable>{ 
 
public void reduce(Text key,Iterable<IntWritable> values, Context 
context) throws IOException,InterruptedException 
 
{ 
 
int sum=0; 
 
for(IntWritable val:values) { 
 
sum += val.get(); 
 
} 
 
context.write(key, new IntWritable(sum)); 
 
} 
 
} 
 
 
 
 
WordCount-------------------------------- 
 
package maximum_log; 
 
import java.io.IOException; 
 
import java.lang.ClassNotFoundException; 
 
import java.net.URI; 
 
import java.util.Scanner; 
 
import org.apache.hadoop.conf.Configuration; 
 
import org.apache.hadoop.fs.FSDataInputStream; 
 
import org.apache.hadoop.fs.FileSystem; 
 
import org.apache.hadoop.fs.Path; 
 
import org.apache.hadoop.io.IntWritable; 
 
import org.apache.hadoop.io.Text; 
 
import org.apache.hadoop.mapreduce.Job; 
 
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; 
 
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; 
 
public class count { 
 
public static void main(String[] args) throws IOException, 
InterruptedException, ClassNotFoundException { 
 
 
Configuration conf = new Configuration(); 
 
Job job=Job.getInstance(conf,"log count"); 
 
job.setJarByClass(count.class); 
 
job.setMapperClass(mapper.class); 
 
job.setReducerClass(reducer.class); 
 
job.setOutputKeyClass(Text.class); 
 
job.setOutputValueClass(IntWritable.class); 
 
FileInputFormat.addInputPath(job, new Path(args[0])); 
 
FileOutputFormat.setOutputPath(job, new Path(args[1])); 
 
job.waitForCompletion(true) ; 
 
 
FileSystem fs = FileSystem.get(URI.create("hdfs://localhost:9000" + 
args[1]),conf); 
 
Path study = fs.listStatus(new Path(args[1]))[1].getPath(); 
 
FSDataInputStream in = null; 
 
in = fs.open(study); 
 
int max = 0; 
 
 
String obj; 
 
Scanner sc = new Scanner(in); 
 
String result = null; 
 
while(sc.hasNext()) { 
 
obj = sc.nextLine(); 
 
String[] arrobj = obj.trim().split("\t+"); 
 
int n = Integer.parseInt(arrobj[1]); 
 
if(n>max) { 
 
max = n; 
result = obj;
 }
 }
 System.out.println(result);
 //sc.close();
 }
 }
 Execution--
hadoop jar '/home/hduser/Download
 /datasett /karanop